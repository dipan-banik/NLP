{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa268eb-6cc1-4b00-80f9-c3dca5372a4c",
   "metadata": {},
   "source": [
    "# Importing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d49d5fa-af49-42a3-9728-37bce3c4464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ae06b",
   "metadata": {},
   "source": [
    "# Byte pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ca89744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, corpus, num_iteration = 8):\n",
    "        punc_removed_corpus = re.sub(r'[^a-zA-Z0-9]', ' ', corpus).lower()      # replacing all the punctuations with space\n",
    "        corpus_vocab = [w+'_' for w in punc_removed_corpus.split()]             # putting '_' at word endings\n",
    "                \n",
    "                \n",
    "        self.BPE_vocab = set()\n",
    "        self.BPE_letters = set()\n",
    "        self.BPE_merges = dict()\n",
    "        self.corpus_vocab_counts = dict()\n",
    "\n",
    "        for word in corpus_vocab:  \n",
    "            self.BPE_vocab.update(word) \n",
    "            self.BPE_letters.update(word)          \n",
    "            word = \" \".join(word)\n",
    "            self.corpus_vocab_counts[word] = self.corpus_vocab_counts.get(word,0) + 1\n",
    "            \n",
    "        self.BPE_vocab.add(\"<UNK>\")\n",
    "\n",
    "                \n",
    "        for _ in tqdm(range(num_iteration)):\n",
    "            \n",
    "            bigram_counts = dict()\n",
    "            for w in self.corpus_vocab_counts.keys():\n",
    "                bigrams = zip(w.split(), w.split()[1:])\n",
    "                for bigram in bigrams:\n",
    "                    bigram_counts[bigram] = bigram_counts.get(bigram, 0) + self.corpus_vocab_counts[w]\n",
    "                    \n",
    "            max_bigram = max(bigram_counts, key=bigram_counts.get)\n",
    "            # print(max_bigram, bigram_counts[max_bigram])\n",
    "\n",
    "            old_keys = list(self.corpus_vocab_counts.keys())\n",
    "            for w in old_keys:\n",
    "                if \" \".join(max_bigram) in w:\n",
    "                    self.corpus_vocab_counts[w.replace(\" \".join(max_bigram), \n",
    "                                                \"\".join(max_bigram), 1)] = self.corpus_vocab_counts.pop(w)\n",
    "            \n",
    "            self.BPE_vocab.add(\"\".join(max_bigram))\n",
    "            self.BPE_merges[max_bigram] = bigram_counts[max_bigram]\n",
    "            \n",
    "            \n",
    "            \n",
    "    def tokenize(self, test_data):\n",
    "        punc_removed_test = re.sub(r'[^a-zA-Z0-9]', ' ', test_data).lower()      # replacing all the punctuations with space\n",
    "        splits = [[l for l in word]+[\"_\"] for word in punc_removed_test.split()]\n",
    "            \n",
    "        for pair in self.BPE_merges:\n",
    "            for idx, split in enumerate(splits):\n",
    "                i = 0\n",
    "                while i < len(split) - 1:\n",
    "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                        split = split[:i] + [\"\".join(pair)] + split[i + 2 :]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                splits[idx] = split\n",
    "\n",
    "        for split in splits:\n",
    "            for i in range(len(split)):\n",
    "                if (len(split[i]) == 1) & (split[i] not in self.BPE_letters):\n",
    "                    split[i] = '<UNK>'\n",
    "                \n",
    "        return splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f2e9c1",
   "metadata": {},
   "source": [
    "## BPE on wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aabf3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = wikipediaapi.Wikipedia('dipanbanik/0.0 (dipanthedataguy@gmail.com)','en')\n",
    "page = wiki.page(\"Natural_language_processing\")\n",
    "\n",
    "corpus = page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85c14f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural language processing (NLP) is a subfield of computer science and especially artificial intell'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aef0295f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 268.79it/s]\n"
     ]
    }
   ],
   "source": [
    "bpe = BPE()\n",
    "bpe.fit(corpus, num_iteration=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f16cd7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('e', '_'): 893,\n",
       " ('s', '_'): 710,\n",
       " ('i', 'n'): 513,\n",
       " ('t', 'h'): 483,\n",
       " ('t', 'i'): 410,\n",
       " ('a', 'n'): 407,\n",
       " ('e', 'n'): 6,\n",
       " ('d', '_'): 380,\n",
       " ('o', 'n'): 350,\n",
       " ('e', 'r'): 311,\n",
       " ('t', '_'): 308,\n",
       " ('o', 'r'): 308,\n",
       " ('a', 'l'): 298,\n",
       " ('y', '_'): 244,\n",
       " ('th', 'e_'): 236,\n",
       " ('g', '_'): 232,\n",
       " ('o', 'f'): 220,\n",
       " ('a', 'r'): 219,\n",
       " ('r', 'e'): 210,\n",
       " ('of', '_'): 208,\n",
       " ('on', '_'): 188,\n",
       " ('t', 'e'): 176,\n",
       " ('a', '_'): 175,\n",
       " ('c', 'h'): 170,\n",
       " ('al', '_'): 165,\n",
       " ('s', 'e'): 157,\n",
       " ('o', '_'): 154,\n",
       " ('in', 'g_'): 154,\n",
       " ('a', 'ti'): 146,\n",
       " ('r', 'o'): 138,\n",
       " ('i', 'c'): 126,\n",
       " ('e', 's_'): 125,\n",
       " ('an', 'd_'): 114,\n",
       " ('t', 'o_'): 108,\n",
       " ('i', 's_'): 107,\n",
       " ('in', '_'): 106,\n",
       " ('g', 'u'): 100,\n",
       " ('c', 'o'): 98,\n",
       " ('d', 'e'): 95,\n",
       " ('u', 'r'): 94,\n",
       " ('l', 'y_'): 90,\n",
       " ('or', '_'): 89,\n",
       " ('l', 'e'): 88,\n",
       " ('a', 'g'): 87,\n",
       " ('a', 'm'): 86,\n",
       " ('p', 'ro'): 86,\n",
       " ('s', 't'): 83,\n",
       " ('ch', '_'): 81,\n",
       " ('a', 't'): 76,\n",
       " ('w', 'or'): 76,\n",
       " ('ati', 'on_'): 75,\n",
       " ('a', 's_'): 74,\n",
       " ('a', 's'): 71,\n",
       " ('s', 'u'): 71,\n",
       " ('c', 'e_'): 71,\n",
       " ('e', 'd_'): 71,\n",
       " ('er', '_'): 69,\n",
       " ('l', 'an'): 69,\n",
       " ('l', 'o'): 68,\n",
       " ('en', '_'): 68,\n",
       " ('lan', 'gu'): 68,\n",
       " ('langu', 'ag'): 68,\n",
       " ('l', 'i'): 64,\n",
       " ('an', '_'): 63,\n",
       " ('m', '_'): 62,\n",
       " ('u', 'n'): 61,\n",
       " ('v', 'e_'): 59,\n",
       " ('a', 'p'): 58,\n",
       " ('d', 'i'): 57,\n",
       " ('d', 's_'): 57,\n",
       " ('te', 'x'): 57,\n",
       " ('s', 'p'): 56,\n",
       " ('s', 'i'): 55,\n",
       " ('languag', 'e_'): 55,\n",
       " ('t', 's_'): 53,\n",
       " ('p', '_'): 52,\n",
       " ('a', 'c'): 52,\n",
       " ('i', 'th'): 52,\n",
       " ('tex', 't_'): 52,\n",
       " ('m', 'p'): 50,\n",
       " ('m', 'e'): 49,\n",
       " ('m', 'an'): 48,\n",
       " ('b', 'e'): 48,\n",
       " ('ur', 'al_'): 48,\n",
       " ('k', '_'): 47,\n",
       " ('n', 'o'): 46,\n",
       " ('e', 'x'): 46,\n",
       " ('w', 'h'): 45,\n",
       " ('ti', 'on_'): 45,\n",
       " ('n', '_'): 42,\n",
       " ('t', 'r'): 42,\n",
       " ('u', 'l'): 41,\n",
       " ('c', 'e'): 41,\n",
       " ('n', 'e'): 40,\n",
       " ('1', '9'): 39,\n",
       " ('b', 'o'): 39,\n",
       " ('sp', 'e'): 39,\n",
       " ('w', 'ith'): 39,\n",
       " ('s', 'y'): 37,\n",
       " ('t', 'o'): 37,\n",
       " ('p', 'ar'): 37,\n",
       " ('n', 'i'): 37,\n",
       " ('s', 'h'): 36,\n",
       " ('m', 'o'): 36,\n",
       " ('f', 'or_'): 36,\n",
       " ('th', 'at_'): 36,\n",
       " ('l', '_'): 35,\n",
       " ('v', 'i'): 35,\n",
       " ('q', 'u'): 35,\n",
       " ('al', 'ly_'): 35,\n",
       " ('n', 'at'): 35,\n",
       " ('ce', 's'): 35,\n",
       " ('se', 'man'): 34,\n",
       " ('with', '_'): 34,\n",
       " ('nat', 'ural_'): 34,\n",
       " ('seman', 'tic'): 34,\n",
       " ('n', 'l'): 33,\n",
       " ('m', 's_'): 33,\n",
       " ('m', 'ar'): 33,\n",
       " ('t', 'as'): 33,\n",
       " ('2', '0'): 32,\n",
       " ('c', 'i'): 32,\n",
       " ('f', 'or'): 32,\n",
       " ('k', 's_'): 32,\n",
       " ('g', 'ni'): 32,\n",
       " ('s', 'o'): 31,\n",
       " ('ati', 'on'): 31,\n",
       " ('wor', 'ds_'): 31,\n",
       " ('nl', 'p_'): 31,\n",
       " ('co', 'gni'): 31,\n",
       " ('a', 'b'): 30,\n",
       " ('ic', 'al_'): 30,\n",
       " ('d', 'u'): 29,\n",
       " ('p', 'o'): 29,\n",
       " ('r', 'i'): 29,\n",
       " ('g', 'i'): 29,\n",
       " ('th', 'o'): 28,\n",
       " ('th', 'er_'): 27,\n",
       " ('co', 'mp'): 27,\n",
       " ('th', 'e'): 26,\n",
       " ('su', 'ch_'): 26,\n",
       " ('v', 'en_'): 26,\n",
       " ('g', 'r'): 26,\n",
       " ('i', '_'): 25,\n",
       " ('gi', 'ven_'): 25,\n",
       " ('an', 's'): 24,\n",
       " ('b', 'y_'): 24,\n",
       " ('en', 'ce_'): 24,\n",
       " ('0', 's_'): 24,\n",
       " ('pro', 'ces'): 24,\n",
       " ('sen', 'te'): 23,\n",
       " ('sente', 'n'): 23,\n",
       " ('v', 'er'): 22,\n",
       " ('t', 'y_'): 22,\n",
       " ('th', 'is_'): 22,\n",
       " ('men', 't_'): 7,\n",
       " ('h', 'o'): 21,\n",
       " ('c', 'l'): 21,\n",
       " ('e', 'ch_'): 21,\n",
       " ('w', 'e'): 20,\n",
       " ('u', 's_'): 20,\n",
       " ('g', 'h'): 20,\n",
       " ('ti', 'f'): 20,\n",
       " ('wor', 'd_'): 20,\n",
       " ('ti', 've_'): 20,\n",
       " ('wh', 'ich_'): 20,\n",
       " ('g', 'ener'): 20,\n",
       " ('spe', 'ech_'): 20,\n",
       " ('p', 'er'): 19,\n",
       " ('b', 'ased_'): 19,\n",
       " ('c', 'an_'): 19,\n",
       " ('ap', 'pro'): 19,\n",
       " ('m', 'ach'): 19,\n",
       " ('for', 'm'): 19,\n",
       " ('tr', 'ans'): 19,\n",
       " ('h', 'a'): 18,\n",
       " ('an', 'al'): 18,\n",
       " ('v', 'e'): 18,\n",
       " ('ti', 'on'): 18,\n",
       " ('f', 'ro'): 18,\n",
       " ('di', 's'): 18,\n",
       " ('sy', 'ste'): 18,\n",
       " ('to', 'k'): 18,\n",
       " ('semantic', '_'): 18,\n",
       " ('tas', 'ks_'): 18,\n",
       " ('o', 'ther_'): 18,\n",
       " ('comp', 'u'): 18,\n",
       " ('gr', 'am'): 18,\n",
       " ('senten', 'ce_'): 18,\n",
       " ('mach', 'ine_'): 18,\n",
       " ('anal', 'y'): 18,\n",
       " ('t', 'y'): 17,\n",
       " ('i', 'l'): 17,\n",
       " ('u', 't_'): 17,\n",
       " ('a', 'u'): 17,\n",
       " ('w', 'as_'): 17,\n",
       " ('g', 'y_'): 17,\n",
       " ('le', 'ar'): 17,\n",
       " ('f', 'ic'): 17,\n",
       " ('mo', 'de'): 17,\n",
       " ('fro', 'm_'): 17,\n",
       " ('ty', 'p'): 17,\n",
       " ('o', 'c'): 16,\n",
       " ('f', 'i'): 16,\n",
       " ('p', 're'): 16,\n",
       " ('par', 'sing_'): 16,\n",
       " ('semantic', 's_'): 16,\n",
       " ('st', 'ati'): 16,\n",
       " ('proces', 'sing_'): 16,\n",
       " ('lo', 'gy_'): 16,\n",
       " ('stati', 's'): 16,\n",
       " ('statis', 't'): 16,\n",
       " ('statist', 'ical_'): 16,\n",
       " ('w', '_'): 15,\n",
       " ('m', 'a'): 15,\n",
       " ('a', 'd'): 15,\n",
       " ('d', 'at'): 15,\n",
       " ('re', 'l'): 15,\n",
       " ('n', 'g_'): 15,\n",
       " ('ar', 'y_'): 15,\n",
       " ('sh', '_'): 15,\n",
       " ('tas', 'k_'): 15,\n",
       " ('i', 'den'): 15,\n",
       " ('cogni', 'tive_'): 15,\n",
       " ('syste', 'ms_'): 15,\n",
       " ('lear', 'ning_'): 15,\n",
       " ('t', 'a'): 14,\n",
       " ('f', 'er'): 14,\n",
       " ('re', 'p'): 14,\n",
       " ('in', 'to_'): 14,\n",
       " ('t', 'u'): 14,\n",
       " ('pro', 'b'): 14,\n",
       " ('no', 't_'): 14,\n",
       " ('ne', 'ural_'): 14,\n",
       " ('spe', 'ci'): 14,\n",
       " ('re', 'cogni'): 14,\n",
       " ('trans', 'l'): 14,\n",
       " ('ha', 've_'): 14,\n",
       " ('th', '_'): 13,\n",
       " ('o', 'p'): 13,\n",
       " ('o', 'l'): 13,\n",
       " ('g', 'e_'): 13,\n",
       " ('en', 'ti'): 13,\n",
       " ('c', 're'): 13,\n",
       " ('u', 'sed_'): 13,\n",
       " ('languag', 'es_'): 13,\n",
       " ('gu', 'i'): 13,\n",
       " ('sin', 'ce_'): 13,\n",
       " ('s', 'si'): 13,\n",
       " ('al', 'so_'): 13,\n",
       " ('gram', 'mar'): 13,\n",
       " ('l', 'a'): 12,\n",
       " ('m', 'i'): 12,\n",
       " ('se', 'ar'): 12,\n",
       " ('e', 'c'): 12,\n",
       " ('str', 'u'): 12,\n",
       " ('z', 'ation_'): 12,\n",
       " ('mor', 'p'): 12,\n",
       " ('20', '0'): 12,\n",
       " ('20', '1'): 12,\n",
       " ('me', 'tho'): 12,\n",
       " ('u', 'gh'): 12,\n",
       " ('appro', 'ach_'): 12,\n",
       " ('analy', 'sis_'): 12,\n",
       " ('rep', 'r'): 12,\n",
       " ('gui', 'stic'): 12,\n",
       " ('grammar', '_'): 12,\n",
       " ('stru', 'c'): 12,\n",
       " ('repr', 'e'): 12,\n",
       " ('f', '_'): 11,\n",
       " ('i', 't_'): 11,\n",
       " ('b', 'i'): 11,\n",
       " ('h', 'i'): 11,\n",
       " ('k', 'e_'): 11,\n",
       " ('z', 'ed_'): 11,\n",
       " ('un', 'der'): 11,\n",
       " ('k', 'no'): 11,\n",
       " ('r', 'ule_'): 11,\n",
       " ('co', 'm'): 11,\n",
       " ('cl', 'u'): 11,\n",
       " ('mode', 'l'): 11,\n",
       " ('de', 've'): 11,\n",
       " ('re', 'sear'): 11,\n",
       " ('lin', 'guistic'): 11,\n",
       " ('kno', 'w'): 11,\n",
       " ('h', 'e'): 10,\n",
       " ('se', 'e_'): 10,\n",
       " ('l', 'd_'): 10,\n",
       " ('le', 'd'): 10,\n",
       " ('ap', 'p'): 10,\n",
       " ('ex', 'amp'): 10,\n",
       " ('ne', 't'): 10,\n",
       " ('19', '9'): 10,\n",
       " ('mo', 'st_'): 10,\n",
       " ('di', 'vi'): 10,\n",
       " ('ex', 'p'): 10,\n",
       " ('dis', 'co'): 10,\n",
       " ('tok', 'en_'): 10,\n",
       " ('stan', 'ding_'): 10,\n",
       " ('sy', 'm'): 10,\n",
       " ('b', 'le_'): 10,\n",
       " ('de', 'ter'): 10,\n",
       " ('metho', 'ds_'): 10,\n",
       " ('ugh', '_'): 10,\n",
       " ('repre', 'sen'): 10,\n",
       " ('resear', 'ch_'): 10,\n",
       " ('linguistic', 's_'): 10,\n",
       " ('net', 'wor'): 10,\n",
       " ('disco', 'ur'): 10,\n",
       " ('c', 'y_'): 9,\n",
       " ('man', 'y_'): 9,\n",
       " ('ex', 'tr'): 9,\n",
       " ('m', 'ul'): 9,\n",
       " ('r', 'ules_'): 9,\n",
       " ('al', 'g'): 9,\n",
       " ('ap', 'i'): 9,\n",
       " ('su', 'b'): 9,\n",
       " ('compu', 'ter_'): 9,\n",
       " ('di', 'f'): 9,\n",
       " ('prob', 'le'): 9,\n",
       " ('recogni', 'tion_'): 9,\n",
       " ('bo', 'th_'): 9,\n",
       " ('in', 'clu'): 9,\n",
       " ('model', 's_'): 9,\n",
       " ('deve', 'lop'): 9,\n",
       " ('examp', 'le_'): 9,\n",
       " ('po', 'ssi'): 9,\n",
       " ('deter', 'mine_'): 9,\n",
       " ('discour', 'se_'): 9,\n",
       " ('alg', 'orith'): 9,\n",
       " ('possi', 'ble_'): 9,\n",
       " ('1', '_'): 8,\n",
       " ('r', '_'): 8,\n",
       " ('a', 'i'): 8,\n",
       " ('i', 'on_'): 8,\n",
       " ('in', 'ten'): 8,\n",
       " ('k', 'ing_'): 8,\n",
       " ('s', 's_'): 8,\n",
       " ('w', 'i'): 8,\n",
       " ('in', 'te'): 8,\n",
       " ('f', 'r'): 8,\n",
       " ('con', 'text_'): 8,\n",
       " ('i', 'mp'): 8,\n",
       " ('se', 'g'): 8,\n",
       " ('ar', 'gu'): 8,\n",
       " ('h', 'u'): 8,\n",
       " ('be', 'en_'): 8,\n",
       " ('re', 'sul'): 8,\n",
       " ('19', '7'): 8,\n",
       " ('19', '8'): 8,\n",
       " ('bo', 'o'): 8,\n",
       " ('al', 'l_'): 8,\n",
       " ('qu', 'i'): 8,\n",
       " ('pro', 'du'): 8,\n",
       " ('c', 'ri'): 8,\n",
       " ('the', 'y_'): 8,\n",
       " ('ho', 'we'): 8,\n",
       " ('typ', 'ic'): 8,\n",
       " ('oc', 'u'): 8,\n",
       " ('fi', 'r'): 8,\n",
       " ('in', 'formation_'): 8,\n",
       " ('re', 'fer'): 8,\n",
       " ('tu', 'al_'): 8,\n",
       " ('speci', 'fic'): 8,\n",
       " ('v', 'ol'): 8,\n",
       " ('identif', 'y_'): 8,\n",
       " ('in', 'cre'): 8,\n",
       " ('j', 'ec'): 8,\n",
       " ('morp', 'hology_'): 8,\n",
       " ('know', 'led'): 8,\n",
       " ('199', '0s_'): 8,\n",
       " ('under', 'standing_'): 8,\n",
       " ('sym', 'bolic'): 8,\n",
       " ('extr', 'ac'): 8,\n",
       " ('c', 'api'): 8,\n",
       " ('inte', 'l'): 8,\n",
       " ('howe', 'ver_'): 8,\n",
       " ('typic', 'ally_'): 8,\n",
       " ('fir', 'st_'): 8,\n",
       " ('knowled', 'ge_'): 8,\n",
       " ('symbolic', '_'): 8,\n",
       " ('intel', 'li'): 8,\n",
       " ('intelli', 'g'): 8,\n",
       " ('0', '_'): 7,\n",
       " ('e', 'ar'): 7,\n",
       " ('en', 't_'): 7,\n",
       " ('d', 'o'): 7,\n",
       " ('de', 'p'): 7,\n",
       " ('of', 'ten_'): 7,\n",
       " ('g', 's_'): 7,\n",
       " ('e', 'ach_'): 7,\n",
       " ('c', 'ur'): 7,\n",
       " ('n', 'amed_'): 7,\n",
       " ('men', 'ts_'): 7,\n",
       " ('be', 'ing_'): 7,\n",
       " ('p', 'e'): 7,\n",
       " ('me', 'aning_'): 7,\n",
       " ('pro', 'vi'): 7,\n",
       " ('qu', 'e'): 7,\n",
       " ('su', 'c'): 7,\n",
       " ('so', 'me_'): 7,\n",
       " ('w', 'ri'): 7,\n",
       " ('the', 'se_'): 7,\n",
       " ('s', 'ci'): 7,\n",
       " ('o', 'us_'): 7,\n",
       " ('gener', 'ation_'): 7,\n",
       " ('b', 'ut_'): 7,\n",
       " ('lo', 'w_'): 7,\n",
       " ('en', 'g'): 7,\n",
       " ('compu', 'tation'): 7,\n",
       " ('me', 'n'): 7,\n",
       " ('am', 'bi'): 7,\n",
       " ('relation', 'shi'): 7,\n",
       " ('appro', 'aches_'): 7,\n",
       " ('in', 'divi'): 7,\n",
       " ('networ', 'ks_'): 7,\n",
       " ('hu', 'man_'): 7,\n",
       " ('boo', 'k_'): 7,\n",
       " ('s', 'cri'): 7,\n",
       " ('in', 'vol'): 7,\n",
       " ('incre', 'asi'): 7,\n",
       " ('capi', 'tali'): 7,\n",
       " ('intellig', 'ence_'): 7,\n",
       " ('eng', 'li'): 7,\n",
       " ('computation', 'al_'): 7,\n",
       " ('ambi', 'gu'): 7,\n",
       " ('indivi', 'du'): 7,\n",
       " ('engli', 'sh_'): 7,\n",
       " ('individu', 'al_'): 7,\n",
       " ('p', 'u'): 6,\n",
       " ('2', '_'): 6,\n",
       " ('9', '_'): 6,\n",
       " ('8', '_'): 6,\n",
       " ('6', '_'): 6,\n",
       " ('p', 'h'): 6,\n",
       " ('an', 'y_'): 6,\n",
       " ('t', 'w'): 6,\n",
       " ('p', 's_'): 6,\n",
       " ('u', 'sing_'): 6,\n",
       " ('t', 'ly_'): 6,\n",
       " ('i', 'ts_'): 6,\n",
       " ('ti', 'me_'): 6,\n",
       " ('n', 'ames_'): 6,\n",
       " ('an', 'no'): 6,\n",
       " ('se', 'par'): 6,\n",
       " ('amo', 'un'): 6,\n",
       " ('ation', 's_'): 6,\n",
       " ('p', 'ri'): 6,\n",
       " ('su', 'm'): 6,\n",
       " ('o', 'b'): 6,\n",
       " ('tion', 'al_'): 6,\n",
       " ('i', 'es_'): 6,\n",
       " ('au', 'to'): 6,\n",
       " ('dic', 'tion'): 6,\n",
       " ('g', 'ly_'): 6,\n",
       " ('sy', 'n'): 6,\n",
       " ('re', 'sol'): 6,\n",
       " ('labe', 'l'): 6,\n",
       " ('aspec', 'ts_'): 6,\n",
       " ('li', 'ke_'): 6,\n",
       " ('wor', 'ld_'): 6,\n",
       " ('tho', 'ugh_'): 6,\n",
       " ('represen', 'tation_'): 6,\n",
       " ('en', 'cy_'): 6,\n",
       " ('mul', 'ti'): 6,\n",
       " ('seg', 'men'): 6,\n",
       " ('chun', 'k_'): 6,\n",
       " ('resul', 'ts_'): 6,\n",
       " ('198', '0s_'): 6,\n",
       " ('u', 'tion_'): 6,\n",
       " ('depen', 'd'): 6,\n",
       " ('scri', 'p'): 6,\n",
       " ('senti', 'ment_'): 6,\n",
       " ('diction', 'ary_'): 6,\n",
       " ('label', 'ling_'): 6,\n",
       " ('segmen', 'tation_'): 6,\n",
       " ('4', '_'): 5,\n",
       " ('3', '_'): 5,\n",
       " ('u', 'se_'): 5,\n",
       " ('sen', 's'): 5,\n",
       " ('m', 'm_'): 5,\n",
       " ('con', 'si'): 5,\n",
       " ('u', 'p_'): 5,\n",
       " ('n', 'u'): 5,\n",
       " ('c', 'tion_'): 5,\n",
       " ('f', 'ul'): 5,\n",
       " ('par', 't_'): 5,\n",
       " ('so', 'me'): 5,\n",
       " ('re', 'spon'): 5,\n",
       " ('z', 'ing_'): 5,\n",
       " ('19', '5'): 5,\n",
       " ('19', '6'): 5,\n",
       " ('senten', 'ces_'): 5,\n",
       " ('o', 'n_'): 5,\n",
       " ('v', 'ari'): 5,\n",
       " ('gh', 't_'): 5,\n",
       " ('su', 'per'): 5,\n",
       " ('per', 'son_'): 5,\n",
       " ('z', 'e_'): 5,\n",
       " ('ar', 'tifici'): 5,\n",
       " ('v', 'an'): 5,\n",
       " ('k', 'i'): 5,\n",
       " ('e', 'speci'): 5,\n",
       " ('tion', 's_'): 5,\n",
       " ('enti', 'ty_'): 5,\n",
       " ('la', 'y'): 5,\n",
       " ('p', 'i'): 5,\n",
       " ('tok', 'eni'): 5,\n",
       " ('hi', 'd'): 5,\n",
       " ('chine', 'se_'): 5,\n",
       " ('dif', 'fic'): 5,\n",
       " ('proble', 'ms_'): 5,\n",
       " ('algorith', 'ms_'): 5,\n",
       " ('ai', 'd_'): 5,\n",
       " ('z', 'at'): 5,\n",
       " ('argu', 'ment_'): 5,\n",
       " ('corefer', 'ence_'): 5,\n",
       " ('specific', '_'): 5,\n",
       " ('docu', 'ment_'): 5,\n",
       " ('pe', 'op'): 5,\n",
       " ('sci', 'ence_'): 5,\n",
       " ('dis', 'ambigu'): 5,\n",
       " ('tw', 'o_'): 5,\n",
       " ('be', 'twe'): 5,\n",
       " ('anno', 'tated_'): 5,\n",
       " ('separ', 'ate_'): 5,\n",
       " ('app', 'lic'): 5,\n",
       " ('enti', 't'): 5,\n",
       " ('auto', 'matic'): 5,\n",
       " ('resol', 'ution_'): 5,\n",
       " ('de', 'scrip'): 5,\n",
       " ('super', 'vi'): 5,\n",
       " ('artifici', 'al_'): 5,\n",
       " ('especi', 'ally_'): 5,\n",
       " ('hid', 'den_'): 5,\n",
       " ('diffic', 'ul'): 5,\n",
       " ('peop', 'le_'): 5,\n",
       " ('betwe', 'en_'): 5,\n",
       " ('applic', 'ations_'): 5,\n",
       " ('entit', 'ies_'): 5,\n",
       " ('descrip', 'tion_'): 5,\n",
       " ('supervi', 'sed_'): 5,\n",
       " ('o', 'u'): 4,\n",
       " ('e', 't'): 4,\n",
       " ('5', '_'): 4,\n",
       " ('g', 'o'): 4,\n",
       " ('re', 'al_'): 4,\n",
       " ('i', 'de'): 4,\n",
       " ('e', 'li'): 4,\n",
       " ('ser', 've_'): 4,\n",
       " ('sin', 'g'): 4,\n",
       " ('de', 'e'): 4,\n",
       " ('r', 'ac'): 4,\n",
       " ('me', 'di'): 4,\n",
       " ('ex', 'ten'): 4,\n",
       " ('j', 'o'): 4,\n",
       " ('tre', 'e_'): 4,\n",
       " ('ce', 'p'): 4,\n",
       " ('mar', 'k'): 4,\n",
       " ('du', 'e_'): 4,\n",
       " ('cor', 'po'): 4,\n",
       " ('tho', 'se_'): 4,\n",
       " ('e', 'ither_'): 4,\n",
       " ('comp', 'lex'): 4,\n",
       " ('ver', 'b'): 4,\n",
       " ('se', 'ver'): 4,\n",
       " ('con', 'ver'): 4,\n",
       " ('gener', 'al_'): 4,\n",
       " ('gener', 'ate_'): 4,\n",
       " ('le', 've'): 4,\n",
       " ('ter', 's_'): 4,\n",
       " ('b', 'u'): 4,\n",
       " ('au', 'thor'): 4,\n",
       " ('le', 'm'): 4,\n",
       " ('gram', 'matical_'): 4,\n",
       " ('f', 'e'): 4,\n",
       " ('i', 'mage_'): 4,\n",
       " ('y', 'ing_'): 4,\n",
       " ('lar', 'g'): 4,\n",
       " ('classi', 'f'): 4,\n",
       " ('mini', 'ng_'): 4,\n",
       " ('di', 'rec'): 4,\n",
       " ('7', '_'): 4,\n",
       " ('struc', 'ture_'): 4,\n",
       " ('struc', 'tures_'): 4,\n",
       " ('bi', 'g_'): 4,\n",
       " ('prob', 'abili'): 4,\n",
       " ('com', 'mon_'): 4,\n",
       " ('stic', '_'): 4,\n",
       " ('fi', 'e'): 4,\n",
       " ('inclu', 'des_'): 4,\n",
       " ('develop', 'ment_'): 4,\n",
       " ('algorith', 'm_'): 4,\n",
       " ('a', 'king_'): 4,\n",
       " ('le', 'ss_'): 4,\n",
       " ('fre', 'e_'): 4,\n",
       " ('impor', 'tan'): 4,\n",
       " ('produ', 'ce_'): 4,\n",
       " ('tex', 'tual_'): 4,\n",
       " ('f', 'fici'): 4,\n",
       " ('dif', 'ferent_'): 4,\n",
       " ('do', 'or_'): 4,\n",
       " ('ac', 'cur'): 4,\n",
       " ('que', 'stion_'): 4,\n",
       " ('tech', 'ni'): 4,\n",
       " ('suc', 'ces'): 4,\n",
       " ('wri', 't'): 4,\n",
       " ('be', 'low_'): 4,\n",
       " ('invol', 'ves_'): 4,\n",
       " ('pu', 'b'): 4,\n",
       " ('relationshi', 'ps_'): 4,\n",
       " ('amoun', 't_'): 4,\n",
       " ('p', 'le_'): 4,\n",
       " ('sum', 'mari'): 4,\n",
       " ('increasi', 'n'): 4,\n",
       " ('syn', 'ta'): 4,\n",
       " ('i', 'ty_'): 4,\n",
       " ('depend', 'ency_'): 4,\n",
       " ('du', 'ction_'): 4,\n",
       " ('respon', 'ding_'): 4,\n",
       " ('vari', 'ous_'): 4,\n",
       " ('ad', 'van'): 4,\n",
       " ('difficul', 't_'): 4,\n",
       " ('boun', 'd'): 4,\n",
       " ('et', 'c'): 4,\n",
       " ('someti', 'm'): 4,\n",
       " ('sing', 'le_'): 4,\n",
       " ('dee', 'p_'): 4,\n",
       " ('ma', 'jor_'): 4,\n",
       " ('corpo', 'r'): 4,\n",
       " ('sever', 'al_'): 4,\n",
       " ('author', 's_'): 4,\n",
       " ('fie', 'ld_'): 4,\n",
       " ('writ', 'ten_'): 4,\n",
       " ('pub', 'li'): 4,\n",
       " ('increasin', 'gly_'): 4,\n",
       " ('synta', 'x'): 4,\n",
       " ('etc', '_'): 4,\n",
       " ('sometim', 'es_'): 4,\n",
       " ('corpor', 'a_'): 4,\n",
       " ('publi', 'shed_'): 4,\n",
       " ('syntax', '_'): 4,\n",
       " ('en', 'd_'): 3,\n",
       " ('d', 'r'): 3,\n",
       " ('a', 'y_'): 3,\n",
       " ('ro', 'o'): 3,\n",
       " ('er', 'ror_'): 3,\n",
       " ('m', 'u'): 3,\n",
       " ('b', 'ase_'): 3,\n",
       " ('c', 'ases_'): 3,\n",
       " ('c', 'al'): 3,\n",
       " ('a', 'f'): 3,\n",
       " ('w', 's_'): 3,\n",
       " ('li', 'st_'): 3,\n",
       " ('ter', 'm_'): 3,\n",
       " ('j', 'ap'): 3,\n",
       " ('f', 'un'): 3,\n",
       " ('e', 'l'): 3,\n",
       " ('g', 'n'): 3,\n",
       " ('f', 'u'): 3,\n",
       " ('f', 'ac'): 3,\n",
       " ('s', 'ame_'): 3,\n",
       " ('ter', 'med_'): 3,\n",
       " ('m', 'ber_'): 3,\n",
       " ('wor', 'k_'): 3,\n",
       " ('tr', 'u'): 3,\n",
       " ('tren', 'ds_'): 3,\n",
       " ('s', 'cene_'): 3,\n",
       " ('con', 'cer'): 3,\n",
       " ('su', 'ally_'): 3,\n",
       " ('ter', 'ms_'): 3,\n",
       " ('be', 'fore_'): 3,\n",
       " ('abo', 've_'): 3,\n",
       " ('re', 'du'): 3,\n",
       " ('spo', 'k'): 3,\n",
       " ('lo', 'gic'): 3,\n",
       " ('b', 'ri'): 3,\n",
       " ('the', 'or'): 3,\n",
       " ('ver', 'y_'): 3,\n",
       " ('k', 'ly_'): 3,\n",
       " ('gener', 'ated_'): 3,\n",
       " ('peri', 'o'): 3,\n",
       " ('re', 'vi'): 3,\n",
       " ('fron', 't_'): 3,\n",
       " ('syste', 'm_'): 3,\n",
       " ('tok', 'en'): 3,\n",
       " ('loc', 'ation_'): 3,\n",
       " ('tic', '_'): 3,\n",
       " ('pre', 'dic'): 3,\n",
       " ('re', 'ad'): 3,\n",
       " ('ani', 'sh_'): 3,\n",
       " ('men', 'tal_'): 3,\n",
       " ('tag', 'ging_'): 3,\n",
       " ('p', 'sy'): 3,\n",
       " ('cre', 'ation_'): 3,\n",
       " ('ste', 'm'): 3,\n",
       " ('li', 'mi'): 3,\n",
       " ('in', 'f'): 3,\n",
       " ('or', 'g'): 3,\n",
       " ('con', 'struc'): 3,\n",
       " ('i', 'f_'): 3,\n",
       " ('com', 'monly_'): 3,\n",
       " ('know', 'n_'): 3,\n",
       " ('experi', 'ence_'): 3,\n",
       " ('exp', 'licit_'): 3,\n",
       " ('oper', 'ation'): 3,\n",
       " ('inclu', 'de_'): 3,\n",
       " ('the', 'i'): 3,\n",
       " ('en', 'tail'): 3,\n",
       " ('wil', 'l_'): 3,\n",
       " ('imp', 'licit_'): 3,\n",
       " ('re', 'qui'): 3,\n",
       " ('refer', 'r'): 3,\n",
       " ('jec', 'tive_'): 3,\n",
       " ('20', '2'): 3,\n",
       " ('y', 'ear'): 3,\n",
       " ('ear', 'ly_'): 3,\n",
       " ('din', 'gs_'): 3,\n",
       " ('se', 'quence_'): 3,\n",
       " ('relationshi', 'p_'): 3,\n",
       " ('capitali', 'zation_'): 3,\n",
       " ('cor', 'pus_'): 3,\n",
       " ('in', 'put_'): 3,\n",
       " ('ph', 'r'): 3,\n",
       " ('gr', 'aph'): 3,\n",
       " ('represen', 'tations_'): 3,\n",
       " ('as', 'sump'): 3,\n",
       " ('al', 'though_'): 3,\n",
       " ('multi', 'mo'): 3,\n",
       " ('sens', 'e_'): 3,\n",
       " ('p', 'mm_'): 3,\n",
       " ('con', 'tinu'): 3,\n",
       " ('nu', 'merical_'): 3,\n",
       " ('cor', 'rection_'): 3,\n",
       " ('recogni', 'zing_'): 3,\n",
       " ('195', '0s_'): 3,\n",
       " ('tur', 'ki'): 3,\n",
       " ('que', 'stions_'): 3,\n",
       " ('lay', 'er_'): 3,\n",
       " ('topic', '_'): 3,\n",
       " ('pi', 'ece_'): 3,\n",
       " ('tokeni', 'zation_'): 3,\n",
       " ('disambigu', 'ation_'): 3,\n",
       " ('w', 'ould_'): 3,\n",
       " ('th', 'rough_'): 3,\n",
       " ('vide', 'o_'): 3,\n",
       " ('eli', 'z'): 3,\n",
       " ('rac', 'ter_'): 3,\n",
       " ('jo', 'h'): 3,\n",
       " ('con', 'cep'): 3,\n",
       " ('v', 'oc'): 3,\n",
       " ('lem', 'mati'): 3,\n",
       " ('larg', 'er_'): 3,\n",
       " ('classif', 'y_'): 3,\n",
       " ('classific', 'ation_'): 3,\n",
       " ('direc', 'tions_'): 3,\n",
       " ('probabili', 'stic_'): 3,\n",
       " ('spe', 'aking_'): 3,\n",
       " ('ffici', 'ent_'): 3,\n",
       " ('accur', 'ate_'): 3,\n",
       " ('techni', 'que_'): 3,\n",
       " ('succes', 's'): 3,\n",
       " ('multi', 'ple_'): 3,\n",
       " ('summari', 'zation_'): 3,\n",
       " ('complex', 'ity_'): 3,\n",
       " ('in', 'tro'): 3,\n",
       " ('cor', 'responding_'): 3,\n",
       " ('bound', 'aries_'): 3,\n",
       " ('clo', 'sely_'): 3,\n",
       " ('tru', 'e_'): 3,\n",
       " ('concer', 'ned_'): 3,\n",
       " ('spok', 'en_'): 3,\n",
       " ('logic', '_'): 3,\n",
       " ('theor', 'y_'): 3,\n",
       " ('token', 's_'): 3,\n",
       " ('predic', 'ates_'): 3,\n",
       " ('stem', 'ming_'): 3,\n",
       " ('inf', 'lec'): 3,\n",
       " ('operation', 'ali'): 3,\n",
       " ('thei', 'r_'): 3,\n",
       " ('referr', 'ed_'): 3,\n",
       " ('year', 's_'): 3,\n",
       " ('multimo', 'd'): 3,\n",
       " ('turki', 'sh_'): 3,\n",
       " ('eliz', 'a_'): 3,\n",
       " ('joh', 'n_'): 3,\n",
       " ('lemmati', 'zat'): 3,\n",
       " ('intro', 'duction_'): 3,\n",
       " ('lemmatizat', 'ion_'): 3,\n",
       " ('v', 's_'): 2,\n",
       " ('k', 'e'): 2,\n",
       " ('p', 'c'): 2,\n",
       " ('th', 're'): 2,\n",
       " ('c', 'are_'): 2,\n",
       " ('f', 'al'): 2,\n",
       " ('z', 'ero_'): 2,\n",
       " ('y', 'es_'): 2,\n",
       " ('co', 'ar'): 2,\n",
       " ('r', 'y_'): 2,\n",
       " ('te', 'st_'): 2,\n",
       " ('b', 'r'): 2,\n",
       " ('e', 'v'): 2,\n",
       " ('are', 'as_'): 2,\n",
       " ('c', 'ase_'): 2,\n",
       " ('en', 'ded_'): 2,\n",
       " ('i', 'b'): 2,\n",
       " ('se', 'e'): 2,\n",
       " ('de', 'si'): 2,\n",
       " ('an', 'ce_'): 2,\n",
       " ('ti', 'mes_'): 2,\n",
       " ('n', 'ame_'): 2,\n",
       " ('me', 'asure_'): 2,\n",
       " ('g', 'er'): 2,\n",
       " ('le', 's'): 2,\n",
       " ('in', 'dex'): 2,\n",
       " ('tre', 'es_'): 2,\n",
       " ('con', 'tr'): 2,\n",
       " ('stre', 'am_'): 2,\n",
       " ('e', 'ded_'): 2,\n",
       " ('ne', 'w'): 2,\n",
       " ('le', 'h'): 2,\n",
       " ('ne', 'g'): 2,\n",
       " ('to', 'w'): 2,\n",
       " ('par', 'se_'): 2,\n",
       " ('par', 'ses_'): 2,\n",
       " ('par', 'ts_'): 2,\n",
       " ('l', 'u'): 2,\n",
       " ('e', 'mo'): 2,\n",
       " ('e', 'qu'): 2,\n",
       " ('conl', 'l_'): 2,\n",
       " ('i', 'te'): 2,\n",
       " ('cogni', 'tion_'): 2,\n",
       " ('en', 'ab'): 2,\n",
       " ('k', 'y_'): 2,\n",
       " ('lex', 'ical_'): 2,\n",
       " ('re', 'stric'): 2,\n",
       " ('compari', 'son_'): 2,\n",
       " ('o', 'ther'): 2,\n",
       " ('gre', 'at_'): 2,\n",
       " ('ver', 'se_'): 2,\n",
       " ('o', 'ver_'): 2,\n",
       " ('si', 'x'): 2,\n",
       " ('clo', 'se_'): 2,\n",
       " ('clo', 'sed_'): 2,\n",
       " ('cli', 'p_'): 2,\n",
       " ('we', 'b'): 2,\n",
       " ('s', 'lo'): 2,\n",
       " ('ans', 'wer'): 2,\n",
       " ('u', 'm_'): 2,\n",
       " ('chat', 'ter'): 2,\n",
       " ('deri', 'ved_'): 2,\n",
       " ('pro', 'grams_'): 2,\n",
       " ('sty', 'le_'): 2,\n",
       " ('stil', 'l_'): 2,\n",
       " ('un', 'til_'): 2,\n",
       " ('lear', 'n_'): 2,\n",
       " ('b', 'loc'): 2,\n",
       " ('fi', 've_'): 2,\n",
       " ('ci', 'al_'): 2,\n",
       " ('pre', 'sented_'): 2,\n",
       " ('no', 'w_'): 2,\n",
       " ('c', 'an'): 2,\n",
       " ('ro', 'ad_'): 2,\n",
       " ('stu', 'd'): 2,\n",
       " ('tur', 'n_'): 2,\n",
       " ('tu', 'ally_'): 2,\n",
       " ('propo', 'sed_'): 2,\n",
       " ('sol', 'ved_'): 2,\n",
       " ('po', 'tenti'): 2,\n",
       " ('r', 'u'): 2,\n",
       " ('er', 's_'): 2,\n",
       " ('p', 'lat'): 2,\n",
       " ('mi', 'd_'): 2,\n",
       " ('on', 's_'): 2,\n",
       " ('pre', 'mi'): 2,\n",
       " ('v', '_'): 2,\n",
       " ('ter', 'minology_'): 2,\n",
       " ('wor', 'd'): 2,\n",
       " ('neces', 's'): 2,\n",
       " ('tech', 'nology_'): 2,\n",
       " ('formali', 'zation_'): 2,\n",
       " ('201', '0s_'): 2,\n",
       " ('p', 'f_'): 2,\n",
       " ('la', 'k'): 2,\n",
       " ('ar', 'abic'): 2,\n",
       " ('be', 'hin'): 2,\n",
       " ('mat', 'ching_'): 2,\n",
       " ('ta', 'ke_'): 2,\n",
       " ('re', 'ali'): 2,\n",
       " ('analy', 'zed_'): 2,\n",
       " ('si', 'o'): 2,\n",
       " ('model', 'ling_'): 2,\n",
       " ('where', 'as_'): 2,\n",
       " ('mat', 'ches_'): 2,\n",
       " ('he', 'ad_'): 2,\n",
       " ('he', 'al'): 2,\n",
       " ('hi', 'gher_'): 2,\n",
       " ('f', 'led'): 2,\n",
       " ('app', 'ly_'): 2,\n",
       " ('app', 'li'): 2,\n",
       " ('experi', 'ment_'): 2,\n",
       " ('s', 'sed_'): 2,\n",
       " ('sym', 'bol'): 2,\n",
       " ('sub', 'task_'): 2,\n",
       " ('201', '1_'): 2,\n",
       " ('si', 'milar_'): 2,\n",
       " ('f', 'ai'): 2,\n",
       " ('a', 'v'): 2,\n",
       " ('identific', 'at'): 2,\n",
       " ('mil', 'lion_'): 2,\n",
       " ('expre', 'ssion_'): 2,\n",
       " ('f', 'ol'): 2,\n",
       " ('intere', 'st_'): 2,\n",
       " ('fr', 'ames_'): 2,\n",
       " ('fr', 'ame'): 2,\n",
       " ('imp', 'ly_'): 2,\n",
       " ('argu', 'men'): 2,\n",
       " ('hur', 'ts_'): 2,\n",
       " ('g', 'ari'): 2,\n",
       " ('resul', 't_'): 2,\n",
       " ('197', '0s_'): 2,\n",
       " ('ac', 'qui'): 2,\n",
       " ('col', 'lo'): 2,\n",
       " ('produ', 'ced_'): 2,\n",
       " ('f', 'ocu'): 2,\n",
       " ('extrac', 't_'): 2,\n",
       " ('p', 'ati'): 2,\n",
       " ('con', 'tent_'): 2,\n",
       " ('n', 'gs_'): 2,\n",
       " ('cur', 'rent_'): 2,\n",
       " ('fr', 'ag'): 2,\n",
       " ('ne', 'uro'): 2,\n",
       " ('inter', 'disci'): 2,\n",
       " ('capitali', 'zed_'): 2,\n",
       " ('ambigu', 'ous_'): 2,\n",
       " ('mani', 'pulating_'): 2,\n",
       " ('200', '2_'): 2,\n",
       " ('0', '9_'): 2,\n",
       " ('199', '9_'): 2,\n",
       " ('201', '8_'): 2,\n",
       " ('197', '8_'): 2,\n",
       " ('200', '6_'): 2,\n",
       " ('an', 'aphor'): 2,\n",
       " ('morphe', 'mes_'): 2,\n",
       " ('exp', 'lici'): 2,\n",
       " ('amoun', 'ts_'): 2,\n",
       " ('pri', 'mary_'): 2,\n",
       " ('ob', 'jec'): 2,\n",
       " ('con', 'sti'): 2,\n",
       " ('multilin', 'gu'): 2,\n",
       " ('200', '3_'): 2,\n",
       " ('sens', 'es_'): 2,\n",
       " ('r', 'mm_'): 2,\n",
       " ('ful', 'ly_'): 2,\n",
       " ('ful', 'l_'): 2,\n",
       " ('196', '0s_'): 2,\n",
       " ('196', '6_'): 2,\n",
       " ('ri', 'ght_'): 2,\n",
       " ('mi', 'ght_'): 2,\n",
       " ('recogni', 'ze_'): 2,\n",
       " ('tokeni', 'zed_'): 2,\n",
       " ('disambigu', 'ate_'): 2,\n",
       " ('noun', 's_'): 2,\n",
       " ('re', 'sour'): 2,\n",
       " ('soun', 'd_'): 2,\n",
       " ('thou', 's'): 2,\n",
       " ('ou', 't'): 2,\n",
       " ('comp', 'lete_'): 2,\n",
       " ('ob', 'solete_'): 2,\n",
       " ('201', '5_'): 2,\n",
       " ('go', 'al_'): 2,\n",
       " ('go', 'ver'): 2,\n",
       " ('c', 'ate'): 2,\n",
       " ('ide', 'a_'): 2,\n",
       " ('ide', 'as_'): 2,\n",
       " ('pi', 'pelines_'): 2,\n",
       " ('medi', 'a_'): 2,\n",
       " ('inter', 'medi'): 2,\n",
       " ('exten', 'sion_'): 2,\n",
       " ('verb', '_'): 2,\n",
       " ('verb', 'al_'): 2,\n",
       " ('conver', 't_'): 2,\n",
       " ('charac', 'ters_'): 2,\n",
       " ('fe', 'atures_'): 2,\n",
       " ('identif', 'ying_'): 2,\n",
       " ('201', '7_'): 2,\n",
       " ('importan', 't_'): 2,\n",
       " ('successi', 've_'): 2,\n",
       " ('uni', 'ver'): 2,\n",
       " ('sub', 'field_'): 2,\n",
       " ('d', 'ay_'): 2,\n",
       " ('mu', 'ch_'): 2,\n",
       " ('cal', 'led_'): 2,\n",
       " ('af', 'ter_'): 2,\n",
       " ('jap', 'ane'): 2,\n",
       " ('elabor', 'ate_'): 2,\n",
       " ('selec', 't_'): 2,\n",
       " ('el', 'se'): 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.BPE_merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a8045d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a_'],\n",
       " ['lo', 't_'],\n",
       " ['of_'],\n",
       " ['ma', 'ch', 'in', 'e_'],\n",
       " ['lear', 'n', 'ing_'],\n",
       " ['alg', 'o', 's_'],\n",
       " ['ar', 'e_'],\n",
       " ['u', 'se', 'd_'],\n",
       " ['in_'],\n",
       " ['nlp_'],\n",
       " ['but_'],\n",
       " ['trans', 'form', 'ers_'],\n",
       " ['ar', 'e_'],\n",
       " ['the_'],\n",
       " ['be', 's', 't_'],\n",
       " ['right_'],\n",
       " ['now_']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data  = \"A lot of Machine Learning algos are used in NLP, but transformers are the best right now.\"\n",
    "\n",
    "tokenized_test_data = bpe.tokenize(test_data)\n",
    "tokenized_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f2c3b",
   "metadata": {},
   "source": [
    "# Wordpiece Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f00241",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPiece:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, corpus, num_iteration = 8):\n",
    "        punc_removed_corpus = re.sub(r'[^a-zA-Z0-9]', ' ', corpus).lower()      # replacing all the punctuations with space\n",
    "        corpus_vocab = [w+'_' for w in punc_removed_corpus.split()]             # putting '_' at word endings\n",
    "                \n",
    "                \n",
    "        self.BPE_vocab = set([chr(i) for i in range(ord('a'), ord('z') + 1)])\n",
    "        self.corpus_vocab_counts = dict()\n",
    "\n",
    "        for word in corpus_vocab:            \n",
    "            word = \" \".join(word)\n",
    "            self.corpus_vocab_counts[word] = self.corpus_vocab_counts.get(word,0) + 1\n",
    "\n",
    "                \n",
    "        for _ in range(num_iteration):\n",
    "            \n",
    "            bigram_counts = dict()\n",
    "            mono_counts = dict()\n",
    "            \n",
    "            for w in self.corpus_vocab_counts.keys():\n",
    "                bigrams = zip(w.split(), w.split()[1:])\n",
    "                for bigram in bigrams:\n",
    "                    bigram_counts[bigram] = bigram_counts.get(bigram, 0) + self.corpus_vocab_counts[w]\n",
    "                    mono_counts[bigram[0]] = mono_counts.get(bigram[0], 0) + self.corpus_vocab_counts[w]\n",
    "                mono_counts[bigram[1]] = mono_counts.get(bigram[1], 0) + self.corpus_vocab_counts[w]\n",
    "            \n",
    "            for bigram in bigram_counts:\n",
    "                bigram_counts[bigram] = bigram_counts[bigram]/(mono_counts[bigram[0]] * mono_counts[bigram[1]])\n",
    "                    \n",
    "            max_bigram = max(bigram_counts, key=bigram_counts.get)\n",
    "            print(max_bigram, bigram_counts[max_bigram])\n",
    "\n",
    "            old_keys = list(self.corpus_vocab_counts.keys())\n",
    "            for w in old_keys:\n",
    "                if \" \".join(max_bigram) in w:\n",
    "                    self.corpus_vocab_counts[w.replace(\" \".join(max_bigram), \n",
    "                                                \"\".join(max_bigram))] = self.corpus_vocab_counts.pop(w)\n",
    "            \n",
    "            self.BPE_vocab.add(\"\".join(max_bigram))      \n",
    "            \n",
    "    def tokenize(self, test_data):\n",
    "        BPE_vocab_sorted = sorted(list(self.BPE_vocab), key=len, reverse=True)\n",
    "        punc_removed_test = re.sub(r'[^a-zA-Z0-9]', ' ', test_data).lower()      # replacing all the punctuations with space\n",
    "        test_vocab = [w+'_' for w in punc_removed_test.split()]\n",
    "\n",
    "        tokenized_test_data = []\n",
    "        for word in test_vocab:\n",
    "            # print(word)\n",
    "            tokenized_word = []\n",
    "            for token in BPE_vocab_sorted:\n",
    "                if word == '':\n",
    "                    break\n",
    "                else:\n",
    "                    if token in word:\n",
    "                        tokenized_word.append(token)\n",
    "                        word = word.replace(token,\"\")\n",
    "                        # print(word)\n",
    "                        \n",
    "                        \n",
    "            tokenized_test_data.append(tokenized_word)\n",
    "            \n",
    "        return tokenized_test_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
